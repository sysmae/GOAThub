import datetime
import uuid
from typing import Any, Dict, List, Optional

import streamlit as st
from langchain.callbacks.streamlit import StreamlitCallbackHandler
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import AIMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

from constant import LANG_OPTIONS, UI_LABELS

# ì–¸ì–´ì½”ë“œ-ì´ë¦„ ë§¤í•‘ ìƒìˆ˜
LANG_CODE_TO_NAME = {v: k.split(" ", 1)[-1] for k, v in LANG_OPTIONS.items()}
# ì˜ˆ: {'ko': 'í•œêµ­ì–´', 'en': 'English', ...}

# ì–¸ì–´ë³„ ì¶”ì²œ ì§ˆë¬¸
suggested_questions_map = {
    "ko": [
        "ì´ ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•µì‹¬ ê°œë…ì„ ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
        "ì‹¤ìƒí™œì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ],
    "en": [
        "What are the main points discussed in this video?",
        "Can you explain the key concepts in more detail?",
        "What are the practical takeaways from this video?",
    ],
    "ja": [
        "ã“ã®å‹•ç”»ã®ä¸»ãªå†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "é‡è¦ãªæ¦‚å¿µã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
        "å®Ÿç”Ÿæ´»ã«ã©ã®ã‚ˆã†ã«å¿œç”¨ã§ãã¾ã™ã‹ï¼Ÿ",
    ],
    "zh": [
        "æœ¬è§†é¢‘çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "è¯·ç®€å•è§£é‡Šä¸€ä¸‹æ ¸å¿ƒæ¦‚å¿µã€‚",
        "æœ‰å“ªäº›å®é™…åº”ç”¨ï¼Ÿ",
    ],
    "fr": [
        "Quels sont les points principaux abordÃ©s dans cette vidÃ©o ?",
        "Pouvez-vous expliquer les concepts clÃ©s plus en dÃ©tail ?",
        "Quelles sont les applications pratiques de cette vidÃ©o ?",
    ],
    "de": [
        "Was sind die Hauptpunkte dieses Videos?",
        "KÃ¶nnen Sie die wichtigsten Konzepte nÃ¤her erlÃ¤utern?",
        "Was sind die praktischen Erkenntnisse aus diesem Video?",
    ],
    "es": [
        "Â¿CuÃ¡les son los puntos principales de este video?",
        "Â¿Puede explicar los conceptos clave con mÃ¡s detalle?",
        "Â¿CuÃ¡les son las aplicaciones prÃ¡cticas de este video?",
    ],
}

# ì–¸ì–´ë³„ ì´ˆê¸° assistant ë©”ì‹œì§€
initial_assistant_message_map = {
    "ko": lambda video_title: f"""ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! "{video_title}" ì˜ìƒì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì˜ìƒì˜ ë‚´ìš©, ìš”ì•½, ëŒ€ë³¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì€ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ë³´ì„¸ìš”:

â€¢ í•µì‹¬ ê°œë… ë° ì£¼ìš” ë‚´ìš©
â€¢ êµ¬ì²´ì  ì„¸ë¶€ì‚¬í•­ì´ë‚˜ ì˜ˆì‹œ
â€¢ ë³µì¡í•œ ì£¼ì œì— ëŒ€í•œ ì„¤ëª…
â€¢ ì‹¤ìƒí™œ ì ìš© ë°©ë²•
â€¢ ê´€ë ¨ ì§ˆë¬¸ì´ë‚˜ ì¸ì‚¬ì´íŠ¸

ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?""",
    "en": lambda video_title: f"""ğŸ‘‹ Hello! I'm your AI assistant for "{video_title}". I've analyzed the video content, summary, and transcript. Feel free to ask me anything about:

â€¢ Key concepts and main points
â€¢ Specific details or examples
â€¢ Clarifications on complex topics
â€¢ Practical applications
â€¢ Related questions or insights

What would you like to know?""",
    "ja": lambda video_title: f"""ğŸ‘‹ ã“ã‚“ã«ã¡ã¯ï¼ã€Œ{video_title}ã€ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚å‹•ç”»ã®å†…å®¹ã€è¦ç´„ã€å­—å¹•ã‚’åˆ†æã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªå•ã‚’è‡ªç”±ã«ã©ã†ãï¼š

â€¢ é‡è¦ãªæ¦‚å¿µã‚„ä¸»ãªãƒã‚¤ãƒ³ãƒˆ
â€¢ å…·ä½“çš„ãªè©³ç´°ã‚„ä¾‹
â€¢ è¤‡é›‘ãªãƒˆãƒ”ãƒƒã‚¯ã®èª¬æ˜
â€¢ å®Ÿç”Ÿæ´»ã§ã®å¿œç”¨æ–¹æ³•
â€¢ é–¢é€£ã™ã‚‹è³ªå•ã‚„æ´å¯Ÿ

ä½•ãŒçŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ""",
    "zh": lambda video_title: f"""ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯â€œ{video_title}â€è§†é¢‘çš„AIåŠ©æ‰‹ã€‚æˆ‘å·²åˆ†æäº†è§†é¢‘å†…å®¹ã€æ‘˜è¦å’Œå­—å¹•ã€‚æ¬¢è¿éšæ—¶æé—®ï¼š

â€¢ å…³é”®æ¦‚å¿µå’Œä¸»è¦å†…å®¹
â€¢ å…·ä½“ç»†èŠ‚æˆ–ç¤ºä¾‹
â€¢ å¤æ‚ä¸»é¢˜çš„è§£é‡Š
â€¢ å®é™…åº”ç”¨
â€¢ ç›¸å…³é—®é¢˜æˆ–è§è§£

ä½ æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ""",
    "fr": lambda video_title: f"""ğŸ‘‹ Bonjour ! Je suis votre assistant IA pour "{video_title}". J'ai analysÃ© le contenu, le rÃ©sumÃ© et la transcription de la vidÃ©o. N'hÃ©sitez pas Ã  poser des questions sur :

â€¢ Concepts clÃ©s et points principaux
â€¢ DÃ©tails spÃ©cifiques ou exemples
â€¢ Explications sur des sujets complexes
â€¢ Applications pratiques
â€¢ Questions ou idÃ©es connexes

Que souhaitez-vous savoir ?""",
    "de": lambda video_title: f"""ğŸ‘‹ Hallo! Ich bin dein KI-Assistent fÃ¼r "{video_title}". Ich habe den Videoinhalt, die Zusammenfassung und das Transkript analysiert. Stelle gerne Fragen zu:

â€¢ Zentrale Konzepte und Hauptpunkte
â€¢ Spezifische Details oder Beispiele
â€¢ ErklÃ¤rungen zu komplexen Themen
â€¢ Praktische Anwendungen
â€¢ Verwandte Fragen oder Erkenntnisse

Was mÃ¶chtest du wissen?""",
    "es": lambda video_title: f"""ğŸ‘‹ Â¡Hola! Soy tu asistente de IA para "{video_title}". He analizado el contenido, el resumen y la transcripciÃ³n del video. SiÃ©ntete libre de preguntar sobre:

â€¢ Conceptos clave y puntos principales
â€¢ Detalles especÃ­ficos o ejemplos
â€¢ Aclaraciones sobre temas complejos
â€¢ Aplicaciones prÃ¡cticas
â€¢ Preguntas o ideas relacionadas

Â¿QuÃ© te gustarÃ­a saber?""",
}


class LangChainChatManager:
    def __init__(self):
        self.memory = ConversationBufferMemory(return_messages=True, memory_key="chat_history")
        self.chat_template = self._create_chat_template()

    def _create_chat_template(self) -> ChatPromptTemplate:
        """ì±„íŒ…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        # ì–¸ì–´ ì„ íƒì— ë”°ë¼ ë‹µë³€ ì–¸ì–´ë¥¼ ë™ì ìœ¼ë¡œ ì§€ì •
        selected_lang = st.session_state.get("selected_lang", "ko")
        lang_name = LANG_CODE_TO_NAME.get(selected_lang, "í•œêµ­ì–´")
        system_prompt = f"""You are an AI assistant that answers questions based on a YouTube video summary and transcript.

Video summary:
{{summary}}

Part of the original transcript:
{{transcript}}

Refer to the above video content and answer the user's questions kindly and specifically.
Your answer must be written in {lang_name}, and provide accurate information related to the video content.
If the question is about information not present in the video, clearly state that fact and provide any helpful information within the scope of the video content."""
        template = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{question}"),
            ]
        )
        return template

    def get_llm_model(self, model_id: str, provider: str, api_key: str):
        """ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if provider == "gemini" or "gemini" in model_id:
            return ChatGoogleGenerativeAI(
                model=model_id,
                google_api_key=api_key,
                temperature=0.2,
                convert_system_message_to_human=True,
            )
        elif provider == "openai" or "gpt" in model_id:
            return ChatOpenAI(model=model_id, openai_api_key=api_key, temperature=0.2)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì: {provider}")

    def create_chain(self, llm):
        """LangChain ì²´ì¸ ìƒì„±"""
        from langchain.chains import ConversationChain

        chain = ConversationChain(
            llm=llm, memory=self.memory, prompt=self.chat_template, verbose=True
        )
        return chain

    def get_response(
        self,
        question: str,
        summary: str,
        transcript: str,
        model_id: str,
        provider: str,
        api_key: str,
    ) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        try:
            llm = self.get_llm_model(model_id, provider, api_key)

            # ì§ì ‘ invoke ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            formatted_prompt = self.chat_template.format_messages(
                summary=summary,
                transcript=transcript[:2000],
                question=question,
                chat_history=self.memory.chat_memory.messages,
            )

            response = llm.invoke(formatted_prompt)

            # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
            self.memory.save_context({"question": question}, {"answer": response.content})

            return response.content

        except Exception as e:
            return f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def get_streaming_response(
        self,
        question: str,
        summary: str,
        transcript: str,
        model_id: str,
        provider: str,
        api_key: str,
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸°"""
        try:
            llm = self.get_llm_model(model_id, provider, api_key)

            formatted_prompt = self.chat_template.format_messages(
                summary=summary,
                transcript=transcript[:2000],
                question=question,
                chat_history=self.memory.chat_memory.messages,
            )

            response_content = ""
            for chunk in llm.stream(formatted_prompt):
                if hasattr(chunk, "content"):
                    response_content += chunk.content
                    yield chunk.content

            # ì™„ë£Œëœ ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥
            self.memory.save_context({"question": question}, {"answer": response_content})

        except Exception as e:
            yield f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def clear_memory(self):
        """ëŒ€í™” ë©”ëª¨ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear()


def render_chat_tab():
    """ì±„íŒ… íƒ­ ë Œë”ë§ ë©”ì¸ í•¨ìˆ˜"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])

    st.subheader(LABELS["chat_tab"])

    # LangChain ì±„íŒ… ë§¤ë‹ˆì € ì´ˆê¸°í™”
    if "langchain_chat_manager" not in st.session_state:
        st.session_state.langchain_chat_manager = LangChainChatManager()

    chat_manager = st.session_state.langchain_chat_manager

    # ì±„íŒ… ìƒíƒœ ì´ˆê¸°í™”
    _initialize_chat_state()

    # ì–¸ì–´ ë° ì˜ìƒ ì œëª© ì •ë³´
    selected_lang = st.session_state.get("selected_lang", "ko")
    video_title = st.session_state.get("video_title", "ìœ íŠœë¸Œ ì˜ìƒ")
    # ì˜ìƒ ì œëª©ì´ ì—†ìœ¼ë©´ video_idë¡œ ëŒ€ì²´
    if not video_title and st.session_state.get("video_id"):
        video_title = st.session_state["video_id"]

    # ì´ˆê¸° assistant ë©”ì‹œì§€ ë° ì¶”ì²œ ì§ˆë¬¸
    initial_msg_func = initial_assistant_message_map.get(
        selected_lang, initial_assistant_message_map["en"]
    )
    initial_msg = initial_msg_func(video_title)
    suggested_questions = suggested_questions_map.get(selected_lang, suggested_questions_map["en"])

    # ìµœì´ˆ assistant ë©”ì‹œì§€ êµì²´ (ìµœì´ˆ 1íšŒë§Œ)
    if st.session_state.chat_messages and st.session_state.chat_messages[0]["id"] == "welcome":
        st.session_state.chat_messages[0]["content"] = initial_msg

    # ì±„íŒ… ì„¤ì • ì˜µì…˜
    _render_chat_settings()

    # ì¶”ì²œ ì§ˆë¬¸ ë²„íŠ¼ UI
    st.markdown(f"**{LABELS['suggested_questions']}**")
    q_cols = st.columns(len(suggested_questions))
    for idx, q in enumerate(suggested_questions):
        if q_cols[idx].button(q, key=f"suggested_q_{idx}"):
            handle_chat_submit(q)

    # ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
    _render_chat_messages()

    # ì±„íŒ… ì…ë ¥ ì¸í„°í˜ì´ìŠ¤
    render_chat_input(chat_manager)

    # AI ì‘ë‹µ ì²˜ë¦¬
    handle_ai_response(chat_manager)

    # --- ì±„íŒ… ì „ì²´ ë…¸ì…˜ ì €ì¥ ë²„íŠ¼ ---
    st.markdown("---")
    if st.button(LABELS["save_chat_notion"], key="save_chat_to_notion"):
        save_chat_to_notion()
        st.success(LABELS["save_chat_notion_success"])


def _initialize_chat_state():
    """ì±„íŒ… ê´€ë ¨ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = [
            {
                "id": "welcome",
                "role": "assistant",
                "content": LABELS["chat_welcome"],
                "timestamp": datetime.datetime.now(),
            }
        ]
    if "chat_input" not in st.session_state:
        st.session_state.chat_input = ""
    if "chat_loading" not in st.session_state:
        st.session_state.chat_loading = False
    if "use_streaming" not in st.session_state:
        st.session_state.use_streaming = True


def _render_chat_settings():
    """ì±„íŒ… ì„¤ì • ì˜µì…˜ ë Œë”ë§"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        use_streaming = st.checkbox(
            LABELS["streaming_response"], value=st.session_state.use_streaming
        )
        st.session_state.use_streaming = use_streaming
    with col2:
        if st.button(LABELS["reset_chat"]):
            st.session_state.langchain_chat_manager.clear_memory()
            st.session_state.chat_messages = [
                {
                    "id": "welcome",
                    "role": "assistant",
                    "content": LABELS["reset_chat_msg"],
                    "timestamp": datetime.datetime.now(),
                }
            ]
            st.rerun()
    with col3:
        memory_info = len(st.session_state.langchain_chat_manager.memory.chat_memory.messages)
        st.info(LABELS["memory_info"].format(memory_info))


def _render_chat_messages():
    """ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥"""
    chat_container = st.container()
    with chat_container:
        for msg in st.session_state.chat_messages:
            align = "user" if msg["role"] == "user" else "assistant"
            with st.chat_message(align):
                st.markdown(msg["content"])


def render_chat_input(chat_manager: LangChainChatManager):
    """ì±„íŒ… ì…ë ¥ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
    # ì±„íŒ… ë¡œë”© ì¤‘ì´ë©´ ì…ë ¥ì°½ì„ ìˆ¨ê¹€
    if st.session_state.get("chat_loading", False):
        return

    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    col1, col2 = st.columns([8, 1])

    # ì…ë ¥ì°½ keyë¥¼ ì±„íŒ… ë©”ì‹œì§€ ê°œìˆ˜ë¡œ ë™ì ìœ¼ë¡œ ì§€ì •
    chat_input_key = f"chat_input_area_{len(st.session_state.get('chat_messages', []))}"

    with col1:
        user_input = st.text_area(
            LABELS["chat_input_placeholder"],
            value=st.session_state.chat_input,
            key=chat_input_key,
            label_visibility="collapsed",
            height=68,
            disabled=st.session_state.chat_loading,
            placeholder=LABELS["chat_input_placeholder"],
        )

    with col2:
        send_btn = st.button(
            LABELS["chat_send_btn"],
            key="chat_send_btn",
            disabled=not user_input.strip() or st.session_state.chat_loading,
            type="primary",
        )

    if user_input.strip() and not st.session_state.chat_loading:
        st.markdown(
            """
        <script>
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && e.ctrlKey) {
                document.querySelector('[data-testid="chat_send_btn"]').click();
            }
        });
        </script>
        """,
            unsafe_allow_html=True,
        )

    if send_btn and user_input.strip():
        handle_chat_submit(user_input.strip())


def handle_chat_submit(question: str):
    """ì±„íŒ… ì „ì†¡ ì²˜ë¦¬"""
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.chat_messages.append(
        {
            "id": f"user-{uuid.uuid4()}",
            "role": "user",
            "content": question,
            "timestamp": datetime.datetime.now(),
        }
    )

    # ì…ë ¥ì°½ ì´ˆê¸°í™” ë° ë¡œë”© ìƒíƒœ ì„¤ì •
    st.session_state.chat_input = ""
    st.session_state.chat_loading = True
    st.rerun()


def handle_ai_response(chat_manager: LangChainChatManager):
    """AI ì‘ë‹µ ì²˜ë¦¬"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    if not st.session_state.chat_loading:
        return

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì‚¬ìš©ì ë©”ì‹œì§€ì¸ì§€ í™•ì¸
    if (
        len(st.session_state.chat_messages) < 1
        or st.session_state.chat_messages[-1]["role"] != "user"
    ):
        st.session_state.chat_loading = False
        return

    # í•„ìš”í•œ ë°ì´í„° í™•ì¸
    required_data = ["summary", "transcript_text", "selected_model_id", "model_provider"]
    missing_data = [key for key in required_data if key not in st.session_state]

    if missing_data:
        error_msg = LABELS["missing_data"].format(", ".join(missing_data))
        add_ai_message(error_msg)
        st.session_state.chat_loading = False
        st.session_state.chat_input = ""  # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        st.rerun()
        return

    # API í‚¤ í™•ì¸
    model_id = st.session_state.selected_model_id
    provider = st.session_state.model_provider
    api_key_name = "gemini_api_key" if "gemini" in model_id else "openai_api_key"
    api_key = st.session_state.get(api_key_name)

    if not api_key:
        error_msg = LABELS["missing_api_key"].format(provider)
        add_ai_message(error_msg)
        st.session_state.chat_loading = False
        st.session_state.chat_input = ""  # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        st.rerun()
        return

    # ì‘ë‹µ ìƒì„±
    question = st.session_state.chat_messages[-1]["content"]
    summary = st.session_state.summary
    transcript = st.session_state.transcript_text

    if st.session_state.use_streaming:
        handle_streaming_response(
            chat_manager, question, summary, transcript, model_id, provider, api_key
        )
    else:
        handle_regular_response(
            chat_manager, question, summary, transcript, model_id, provider, api_key
        )
    # ì‘ë‹µ í›„ ì…ë ¥ì°½ ë¹„ìš°ê¸°
    st.session_state.chat_input = ""


def handle_streaming_response(
    chat_manager: LangChainChatManager,
    question: str,
    summary: str,
    transcript: str,
    model_id: str,
    provider: str,
    api_key: str,
):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    try:
        # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ placeholder ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in chat_manager.get_streaming_response(
                question, summary, transcript, model_id, provider, api_key
            ):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")

            # ìµœì¢… ì‘ë‹µ í‘œì‹œ
            message_placeholder.markdown(full_response)

        # ì„¸ì…˜ ìƒíƒœì— AI ì‘ë‹µ ì¶”ê°€
        add_ai_message(full_response)

    except Exception as e:
        error_msg = f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        add_ai_message(error_msg)

    finally:
        st.session_state.chat_loading = False
        st.rerun()


def handle_regular_response(
    chat_manager: LangChainChatManager,
    question: str,
    summary: str,
    transcript: str,
    model_id: str,
    provider: str,
    api_key: str,
):
    """ì¼ë°˜ ì‘ë‹µ ì²˜ë¦¬"""
    selected_lang = st.session_state.get("selected_lang", "ko")
    LABELS = UI_LABELS.get(selected_lang, UI_LABELS["ko"])
    try:
        with st.spinner(LABELS["chat_loading"]):
            response = chat_manager.get_response(
                question, summary, transcript, model_id, provider, api_key
            )

        add_ai_message(response)

    except Exception as e:
        error_msg = f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        add_ai_message(error_msg)

    finally:
        st.session_state.chat_loading = False
        st.rerun()


def add_ai_message(content: str):
    """AI ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€"""
    st.session_state.chat_messages.append(
        {
            "id": f"ai-{uuid.uuid4()}",
            "role": "assistant",
            "content": content,
            "timestamp": datetime.datetime.now(),
        }
    )


def save_chat_to_notion():
    """ì±„íŒ… ì „ì²´ë¥¼ Notionì— ì €ì¥"""
    from notion_utils import save_to_notion_as_page

    # ì±„íŒ… ë©”ì‹œì§€ í¬ë§·íŒ…
    chat_msgs = st.session_state.get("chat_messages", [])
    lines = []
    for msg in chat_msgs:
        role = "ğŸ™‹â€â™‚ï¸ ì‚¬ìš©ì" if msg["role"] == "user" else "ğŸ¤– AI"
        ts = msg["timestamp"].strftime("%Y-%m-%d %H:%M")
        lines.append(f"**{role}** ({ts})\n\n{msg['content']}\n")
    chat_md = "\n---\n".join(lines)
    save_to_notion_as_page(chat_md)
